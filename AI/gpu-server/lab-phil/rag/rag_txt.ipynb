{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GiQs8-_Nn_Ml"
   },
   "outputs": [],
   "source": [
    "!pip install -q langchain pypdf sentence-transformers chromadb openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VgYrF6FbNkJL",
    "outputId": "825dd743-7700-45c3-dee2-5f6e79ada8b9"
   },
   "outputs": [],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_HBhuL23S_YR"
   },
   "source": [
    "## Multi-Query Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "import re\n",
    "from typing import Iterator\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.document_loaders.helpers import detect_file_encodings\n",
    "import pandas as pd\n",
    "from langchain_community.document_loaders import CSVLoader\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "class KaKaoTalkLoader(CSVLoader):\n",
    "    def __init__(self, file_path: str, file_suffix:str, encoding: str = \"utf8\", **kwargs):\n",
    "        super().__init__(file_path, encoding=encoding, **kwargs)\n",
    "        # NOTE - choh(2024.04.05) - 파일 확장자 변수 추가\n",
    "        self.file_suffix = file_suffix\n",
    "    \n",
    "    def anonymize_user_id(self, user_id, num_chars_to_anonymize=3):\n",
    "        \"\"\"\n",
    "        비식별화 함수는 주어진 사용자 ID의 앞부분을 '*'로 대체하여 비식별화합니다.\n",
    "\n",
    "        :param user_id: 비식별화할 사용자 ID\n",
    "        :param num_chars_to_anonymize: 비식별화할 문자 수\n",
    "        :return: 비식별화된 사용자 ID\n",
    "        \"\"\"\n",
    "        # 비식별화할 문자 수가 사용자 ID의 길이보다 길 경우, 전체 ID를 '*'로 대체\n",
    "        if num_chars_to_anonymize >= len(user_id):\n",
    "            num_chars_to_anonymize = len(user_id) - 1\n",
    "            return \"*\" * num_chars_to_anonymize\n",
    "\n",
    "        # 앞부분을 '*'로 대체하고 나머지 부분을 원본 ID에서 가져옴\n",
    "        anonymized_id = \"*\" * num_chars_to_anonymize + user_id[num_chars_to_anonymize:]\n",
    "\n",
    "        return anonymized_id\n",
    "    \n",
    "    # NOTE - choh(2024.04.05) - 12시간제를 24시간제로 변환\n",
    "    def process_time_to_24hr_format(self, date_obj, time_str):\n",
    "        \"\"\"\n",
    "        대화 내용중에 시간 표시가 '오전 12:23', '오후 11:23'과 같이 12시간제로 되어 있는 경우, \n",
    "        이를 24시간제로 변환합니다.\n",
    "        \n",
    "        :param date_obj: 대화 내용의 날짜 정보가 담긴 datetime 객체\n",
    "        :praam time_str: 대화 내용의 시간 정보가 담긴 문자열\n",
    "        :return: 24시간제로 변환된 datetime 객체\n",
    "        \"\"\"\n",
    "        \n",
    "        # '오전/오후' 부분과 시간 부분을 분리합니다.\n",
    "        period, time_part = time_str.split(' ', 1)\n",
    "        \n",
    "        # 시간 부분을 시와 분으로 다시 분리합니다.\n",
    "        hour, minute = map(int, time_part.split(':'))\n",
    "        \n",
    "        # '오후'인 경우 12를 더하되, '오후 12시'는 제외합니다.\n",
    "        if period == '오후' and hour != 12:\n",
    "            hour += 12\n",
    "        # '오전 12시'는 0시로 처리합니다.\n",
    "        elif period == '오전' and hour == 12:\n",
    "            hour = 0\n",
    "        \n",
    "        # date_obj과 결합하여 최종 datetime 객체를 생성합니다.\n",
    "        # 여기서 datetime 함수는 위에서 임포트한 datetime 클래스를 사용합니다.\n",
    "        combined_datetime = datetime(date_obj.year, date_obj.month, date_obj.day, hour, minute)\n",
    "        \n",
    "        # pandas의 to_datetime 함수를 사용하여 pandas.Timestamp 객체로 변환합니다.\n",
    "        return pd.to_datetime(combined_datetime)\n",
    "    \n",
    "    # NOTE - choh(2024.04.05) - 대화목록의 날짜 변환 부분을 파싱\n",
    "    def process_date(self, line: str) -> tuple:\n",
    "        \"\"\"\n",
    "        -------- 2024년 4월 5일 화요일 -------- 형태의 날짜를 파싱하고,\n",
    "        파싱 성공 여부와 함께 파싱된 날짜 또는 원래 문자열을 반환합니다.\n",
    "        \n",
    "        :param line: 날짜 문자열\n",
    "        :return: (파싱 성공 여부, 파싱된 날짜 또는 원래 문자열)\n",
    "        \"\"\"\n",
    "        # -------- 2024년 4월 5일 화요일 -------- 날짜가 이상태임\n",
    "        date_match = re.match(r'[-]+ (\\d+년 \\d+월 \\d+일) [^\\d]+', line)\n",
    "        if date_match:\n",
    "            # 2024년 4월 5일, 형태의 날짜 추출\n",
    "            date_pattern = re.compile(r'(\\d+)년 (\\d+)월 (\\d+)일')\n",
    "            match = date_pattern.search(date_match.group(1))\n",
    "            if match:\n",
    "                year, month, day = map(int, match.groups())\n",
    "                return (True, pd.to_datetime(f\"{year}-{month}-{day}\"))\n",
    "        return (False, line)\n",
    "\n",
    "    # NOTE - choh(2024.04.05) - __read_file을 테스트 하기 위한 wrapper 함수\n",
    "    def _read_file_test(self, csvfile) -> Iterator[Document]:\n",
    "        \"\"\"테스트를 위한 래퍼 함수\"\"\"\n",
    "        return self.__read_file(csvfile)\n",
    "    \n",
    "    def __read_file(self, csvfile) -> Iterator[Document]:\n",
    "        # NOTE - choh(2024.04.05) - TXT 형태의 대화 메세지 사전 처리\n",
    "        if self.file_suffix == \".txt\":\n",
    "            \n",
    "            # 전날 날짜 변수 초기화\n",
    "            temp_date = None\n",
    "            i = 0 # 행 번호\n",
    "            for line in csvfile:\n",
    "                \n",
    "                # 이번 줄이 날짜가 맞으면 is_parsed=True, result는 날짜\n",
    "                is_parsed, result = self.process_date(line)\n",
    "                \n",
    "                # 파싱한 문자열이 날짜 패턴에 맞으면, 날짜를 저장\n",
    "                if is_parsed:\n",
    "                    temp_date = result\n",
    "                \n",
    "                # 날짜가 아니면, 체팅이기 때문에, 체팅을 패턴 매칭\n",
    "                else:\n",
    "                    # 초기값 설정\n",
    "                    user = None\n",
    "                    time_12hr = None\n",
    "                    message = None\n",
    "\n",
    "                    # 대화 패턴 찾기\n",
    "                    conversation_match = re.match(r'\\[([^\\]]+)\\] \\[([^\\]]+)\\] (.+)', line)\n",
    "                    if conversation_match:\n",
    "                        user_real = conversation_match.group(1)\n",
    "                        time_12hr = conversation_match.group(2)\n",
    "                        message = conversation_match.group(3).strip()\n",
    "                        \n",
    "                        # 시간을 24시간제로 변환                        \n",
    "                        date = self.process_time_to_24hr_format(temp_date, time_12hr)\n",
    "                        # 사용자 ID 비식별화\n",
    "                        user = self.anonymize_user_id(user_real)\n",
    "                        \n",
    "                        content = f'\"User: {user}, Message: {message}'\n",
    "                        \n",
    "                        metadata = {\n",
    "                            \"date\":  date.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                            \"year\": date.year,\n",
    "                            \"month\": date.month,\n",
    "                            \"day\": date.day,\n",
    "                            \"user\": user,\n",
    "                            \"row\": i,\n",
    "                            \"source\": str(self.file_path),\n",
    "                        }\n",
    "                        i += 1 # 행 번호 증가\n",
    "                        yield Document(page_content=content, metadata=metadata)\n",
    "       \n",
    "        \n",
    "        # NOTE - choh(2024.04.05) - 기존 코드, csv 파일인 경우\n",
    "        else:\n",
    "            df = pd.read_csv(csvfile)\n",
    "            df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "            df[\"Date_strf\"] = df[\"Date\"].dt.strftime(\"%Y-%m-%d %H:%M:%S\").astype(str)\n",
    "            for i, row in df.iterrows():\n",
    "                date = row[\"Date\"]\n",
    "                user = self.anonymize_user_id(row[\"User\"])\n",
    "                content = f'\"User: {user}, Message: {row[\"Message\"]}'\n",
    "\n",
    "                metadata = {\n",
    "                    \"date\": row[\"Date_strf\"],\n",
    "                    \"year\": date.year,\n",
    "                    \"month\": date.month,\n",
    "                    \"day\": date.day,\n",
    "                    \"user\": user,\n",
    "                    \"row\": i,\n",
    "                    \"source\": str(self.file_path),\n",
    "                }\n",
    "                yield Document(page_content=content, metadata=metadata)\n",
    "\n",
    "    def lazy_load(self) -> Iterator[Document]:\n",
    "        try:\n",
    "            with open(self.file_path, newline=\"\", encoding=self.encoding) as csvfile:\n",
    "                yield from self.__read_file(csvfile)\n",
    "      \n",
    "        except UnicodeDecodeError as e:\n",
    "            if self.autodetect_encoding:\n",
    "                detected_encodings = detect_file_encodings(self.file_path)\n",
    "                for encoding in detected_encodings:\n",
    "                    try:\n",
    "                        with open(\n",
    "                            self.file_path, newline=\"\", encoding=encoding.encoding\n",
    "                        ) as csvfile:\n",
    "                            yield from self.__read_file(csvfile)\n",
    "                            break\n",
    "                    except UnicodeDecodeError:\n",
    "                        continue\n",
    "            else:\n",
    "                raise RuntimeError(f\"Error loading {self.file_path}\") from e\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error loading {self.file_path}\") from e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y3TWw12cSqRw"
   },
   "outputs": [],
   "source": [
    "# Build a sample vectorDB\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "import kakaotalk_loader as kakao\n",
    "\n",
    "file_path = \"KakaoTalk_group.txt\"\n",
    "file_suffix = \".txt\"  # Change to \".csv\" if you're using a CSV file\n",
    "loader = kakao.KaKaoTalkLoader(file_path, file_suffix, encoding=\"utf8\")\n",
    "\n",
    "# Load blog post\n",
    "loader = WebBaseLoader(\"https://corin-e.tistory.com/entry/1-%EC%82%BC%EC%84%B1-%EC%B2%AD%EB%85%84-SW-%EC%95%84%EC%B9%B4%EB%8D%B0%EB%AF%B8%EC%8B%B8%ED%94%BC-SSAFY-%EC%82%BC%EC%88%98-%ED%9B%84%EA%B8%B0\")\n",
    "data = loader.load()\n",
    "\n",
    "# Split\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=30)\n",
    "splits = text_splitter.split_documents(data)\n",
    "\n",
    "# VectorDB\n",
    "model_name = \"jhgan/ko-sbert-nli\"\n",
    "encode_kwargs = {'normalize_embeddings': True}\n",
    "ko_embedding = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n",
    "open_key = ''\n",
    "vectordb = Chroma.from_documents(documents=splits, embedding=ko_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "epoTy2FNSuqz"
   },
   "outputs": [],
   "source": [
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "question = \"작성자의 생각\"\n",
    "\n",
    "\n",
    "# llm = ChatOpenAI(\n",
    "#     temperature=0,\n",
    "#     openai_api_key = open_key)\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0,\n",
    "    base_url=\"https://magnetic-ram-brave.ngrok-free.app/v1\",\n",
    "    openai_api_key = \"lm-studio\",\n",
    "    model=\"mradermacher/Llama-3.1-Korean-8B-Instruct-GGUF\",\n",
    "    # streaming=True,\n",
    "    # callbacks=[StreamingStdOutCallbackHandler()],\n",
    ")\n",
    "retriever_from_llm = MultiQueryRetriever.from_llm(\n",
    "    retriever=vectordb.as_retriever(), llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eU6X_HctSumR"
   },
   "outputs": [],
   "source": [
    "# Set logging for the queries\n",
    "import logging\n",
    "\n",
    "logging.basicConfig()\n",
    "logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pXYmox2MSuZ-"
   },
   "outputs": [],
   "source": [
    "unique_docs = retriever_from_llm.get_relevant_documents(query=question)\n",
    "len(unique_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MIz2l4OaS75w"
   },
   "outputs": [],
   "source": [
    "unique_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "digNJMyRtM3k"
   },
   "source": [
    "## 기본 Parent-document Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tZOk-3LOnb-0"
   },
   "outputs": [],
   "source": [
    "from langchain.retrievers import ParentDocumentRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f0ONC3wZn62e"
   },
   "outputs": [],
   "source": [
    "from langchain.storage import InMemoryStore\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VpJhZWThn7gG"
   },
   "outputs": [],
   "source": [
    "loaders = [\n",
    "    TextLoader('KakaoTalk_group.txt')\n",
    "    # PyPDFLoader(\"/content/drive/MyDrive/강의 자료/[복지이슈 FOCUS 15ȣ] 경기도 극저신용대출심사모형 개발을 위한 국내 신용정보 활용가능성 탐색.pdf\"),\n",
    "    # PyPDFLoader(\"/content/drive/MyDrive/강의 자료/[이슈리포트 2022-2호] 혁신성장 정책금융 동향.pdf\"),\n",
    "]\n",
    "docs = []\n",
    "for loader in loaders:\n",
    "    docs.extend(loader.load_and_split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KBb-1ym5pKRG"
   },
   "outputs": [],
   "source": [
    "model_name = \"jhgan/ko-sbert-nli\"\n",
    "encode_kwargs = {'normalize_embeddings': True}\n",
    "ko_embedding = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fyixQa5SpHOW"
   },
   "outputs": [],
   "source": [
    "# This text splitter is used to create the child documents\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=200)\n",
    "# The vectorstore to use to index the child chunks\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"full_documents\", embedding_function=ko_embedding\n",
    ")\n",
    "# The storage layer for the parent documents\n",
    "store = InMemoryStore()\n",
    "retriever = ParentDocumentRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IyVXzfq0pHME"
   },
   "outputs": [],
   "source": [
    "retriever.add_documents(docs, ids=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L8z2B2rKrAv3"
   },
   "outputs": [],
   "source": [
    "sub_docs = vectorstore.similarity_search(\"7월 29일 이후의 대화 내용\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_axFoMvnrA-h"
   },
   "outputs": [],
   "source": [
    "print(\"글 길이: {}\\n\\n\".format(len(sub_docs[0].page_content)))\n",
    "print(sub_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W68G7ITorThj"
   },
   "outputs": [],
   "source": [
    "retrieved_docs = retriever.get_relevant_documents(\"7월 29일 대화 내용\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uzl334J8rXEQ"
   },
   "outputs": [],
   "source": [
    "print(\"글 길이: {}\\n\\n\".format(len(retrieved_docs[0].page_content)))\n",
    "print(retrieved_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AK752NU_tHjb"
   },
   "source": [
    "## 본문의 Full_chunk가 너무 길때"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q64Iw5Wbrd94"
   },
   "outputs": [],
   "source": [
    "# This text splitter is used to create the parent documents\n",
    "parent_splitter = RecursiveCharacterTextSplitter(chunk_size=800)\n",
    "# This text splitter is used to create the child documents\n",
    "# It should create documents smaller than the parent\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=200)\n",
    "# The vectorstore to use to index the child chunks\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"split_parents\", embedding_function=ko_embedding\n",
    ")\n",
    "# The storage layer for the parent documents\n",
    "store = InMemoryStore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1TVbBTmgr_CA"
   },
   "outputs": [],
   "source": [
    "retriever = ParentDocumentRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter,\n",
    "    parent_splitter=parent_splitter,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K9bPu58hsAfP"
   },
   "outputs": [],
   "source": [
    "retriever.add_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KLvoTdeksUne"
   },
   "outputs": [],
   "source": [
    "len(list(store.yield_keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K0Te96C7sVxA"
   },
   "outputs": [],
   "source": [
    "sub_docs = vectorstore.similarity_search(\"7월 29일 대화 내용\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qJ6Qb_E_sZCb"
   },
   "outputs": [],
   "source": [
    "print(sub_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GXioc_E3so_U"
   },
   "outputs": [],
   "source": [
    "len(sub_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bL6F-aBJsZyv"
   },
   "outputs": [],
   "source": [
    "retrieved_docs = retriever.get_relevant_documents(\"7월 29일 대화 내용\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EnhD6dY_sfKq"
   },
   "outputs": [],
   "source": [
    "print(retrieved_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UhxrZ1zAscVP"
   },
   "outputs": [],
   "source": [
    "len(retrieved_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FoTL9eBctWva"
   },
   "source": [
    "## Self-querying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_DmHA3cgtZcJ"
   },
   "outputs": [],
   "source": [
    "!pip install lark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7TJLhLfQthCt"
   },
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "docs = [\n",
    "    Document(\n",
    "        page_content=\"A bunch of scientists bring back dinosaurs and mayhem breaks loose\",\n",
    "        metadata={\"year\": 1993, \"rating\": 7.7, \"genre\": \"science fiction\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Leo DiCaprio gets lost in a dream within a dream within a dream within a ...\",\n",
    "        metadata={\"year\": 2010, \"director\": \"Christopher Nolan\", \"rating\": 8.2},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea\",\n",
    "        metadata={\"year\": 2006, \"director\": \"Satoshi Kon\", \"rating\": 8.6},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"A bunch of normal-sized women are supremely wholesome and some men pine after them\",\n",
    "        metadata={\"year\": 2019, \"director\": \"Greta Gerwig\", \"rating\": 8.3},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Toys come alive and have a blast doing so\",\n",
    "        metadata={\"year\": 1995, \"genre\": \"animated\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Three men walk into the Zone, three men walk out of the Zone\",\n",
    "        metadata={\n",
    "            \"year\": 1979,\n",
    "            \"director\": \"Andrei Tarkovsky\",\n",
    "            \"genre\": \"thriller\",\n",
    "            \"rating\": 9.9,\n",
    "        },\n",
    "    ),\n",
    "]\n",
    "vectorstore = Chroma.from_documents(docs, ko_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9x2PsJ9e7tuH"
   },
   "outputs": [],
   "source": [
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "metadata_field_info = [\n",
    "    AttributeInfo(\n",
    "        name=\"genre\",\n",
    "        description=\"The genre of the movie. One of ['science fiction', 'comedy', 'drama', 'thriller', 'romance', 'action', 'animated']\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"year\",\n",
    "        description=\"The year the movie was released\",\n",
    "        type=\"integer\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"director\",\n",
    "        description=\"The name of the movie director\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"rating\", description=\"A 1-10 rating for the movie\", type=\"float\"\n",
    "    ),\n",
    "]\n",
    "document_content_description = \"Brief summary of a movie\"\n",
    "llm = ChatOpenAI(temperature=0, openai_api_key = \"\")\n",
    "retriever = SelfQueryRetriever.from_llm(\n",
    "    llm,\n",
    "    vectorstore,\n",
    "    document_content_description,\n",
    "    metadata_field_info,\n",
    "    verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "foYY4IEA74Dr"
   },
   "outputs": [],
   "source": [
    "retriever.get_relevant_documents(\"what are some movies rated higher than 8.5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zQP9iztA8ApB"
   },
   "source": [
    "## Time-weighted vector store Retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OKEps8vK8GIm"
   },
   "source": [
    "Scoring 방법 = *semantic_similarity + (1.0 - decay_rate) ^ hours_passed*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zKL8WJ_P8UEG"
   },
   "outputs": [],
   "source": [
    "!pip install -q faiss-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uLG48BJY8AEW"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "import faiss\n",
    "from langchain.docstore import InMemoryDocstore\n",
    "from langchain.retrievers import TimeWeightedVectorStoreRetriever\n",
    "from langchain.schema import Document\n",
    "from langchain_community.vectorstores import FAISS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lv5TtUnK8Mml"
   },
   "outputs": [],
   "source": [
    "# Initialize the vectorstore as empty\n",
    "embedding_size = 768\n",
    "index = faiss.IndexFlatL2(embedding_size)\n",
    "vectorstore = FAISS(ko_embedding, index, InMemoryDocstore({}), {})\n",
    "retriever = TimeWeightedVectorStoreRetriever(\n",
    "    vectorstore=vectorstore, decay_rate=0.99, k=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vL-HP-9L8OIq"
   },
   "outputs": [],
   "source": [
    "yesterday = datetime.now() - timedelta(days=1)\n",
    "retriever.add_documents(\n",
    "    [Document(page_content=\"영어는 훌륭합니다.\", metadata={\"last_accessed_at\": yesterday})]\n",
    ")\n",
    "retriever.add_documents([Document(page_content=\"한국어는 훌륭합니다\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q6KCbXXV_d4F"
   },
   "outputs": [],
   "source": [
    "# \"Hello World\" is returned first because it is most salient, and the decay rate is close to 0., meaning it's still recent enough\n",
    "retriever.get_relevant_documents(\"영어가 좋아요\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sjtkFgOGfemK"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ltRw8HCmmECS"
   },
   "source": [
    "## Ensemble Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QN9Qk5_LkeQH"
   },
   "outputs": [],
   "source": [
    "!pip install -q langchain pypdf sentence-transformers chromadb langchain-openai faiss-gpu --upgrade --quiet  rank_bm25 > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h7HV_yPpmKdX"
   },
   "outputs": [],
   "source": [
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader('KakaoTalk_group.txt')\n",
    "data = loader.load()\n",
    "\n",
    "print(type(data))\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader('KakaoTalk_group.txt')\n",
    "data = loader.load()\n",
    "\n",
    "print(type(data))\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OYhREH6zmMJp"
   },
   "outputs": [],
   "source": [
    "model_name = \"jhgan/ko-sbert-nli\"\n",
    "encode_kwargs = {'normalize_embeddings': True}\n",
    "ko_embedding = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "15n17ed7meJI"
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "loaders = [\n",
    "    # PyPDFLoader(\"./first.pdf\"),\n",
    "    # PyPDFLoader(\"./fsecond.pdf\"),\n",
    "    TextLoader('KakaoTalk_group.txt')\n",
    "]\n",
    "print(loaders)\n",
    "docs = []\n",
    "for loader in loaders:\n",
    "    docs.extend(loader.load_and_split())\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=50)\n",
    "texts = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ck2cUaLX4pa3"
   },
   "outputs": [],
   "source": [
    "# initialize the bm25 retriever and faiss retriever\n",
    "bm25_retriever = BM25Retriever.from_documents(texts)\n",
    "bm25_retriever.k = 2\n",
    "\n",
    "\n",
    "\n",
    "embedding = ko_embedding\n",
    "faiss_vectorstore = FAISS.from_documents(texts, ko_embedding)\n",
    "faiss_retriever = faiss_vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
    "\n",
    "# initialize the ensemble retriever\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[bm25_retriever, faiss_retriever], weights=[0.5, 0.5]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JcBSDvy3mmlx"
   },
   "outputs": [],
   "source": [
    "# docs = ensemble_retriever.invoke(\"혁신정책금융과 극저신용대출모형의 차이\")\n",
    "docs = ensemble_retriever.invoke(\"7월 29일 대화내용\")\n",
    "for i in docs:\n",
    "\n",
    "  print(i.metadata)\n",
    "  print(\":\")\n",
    "  print(i.page_content)\n",
    "  print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c0Jhox_l6qe2"
   },
   "outputs": [],
   "source": [
    "faiss_vectorstore = FAISS.from_documents(texts, ko_embedding)\n",
    "faiss_retriever = faiss_vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
    "\n",
    "docs = faiss_retriever.invoke(\"7월 29일 대화내용\")\n",
    "for i in docs:\n",
    "\n",
    "  print(i.metadata)\n",
    "  print(\":\")\n",
    "  print(i.page_content)\n",
    "  print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "28vloMiumuY_"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = ''\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# openai = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature = 0)\n",
    "openai = ChatOpenAI(\n",
    "    temperature=0,\n",
    "    base_url=\"https://magnetic-ram-brave.ngrok-free.app/v1\",\n",
    "    openai_api_key = \"lm-studio\",\n",
    "    model=\"mradermacher/Llama-3.1-Korean-8B-Instruct-GGUF\",\n",
    "    # streaming=True,\n",
    "    # callbacks=[StreamingStdOutCallbackHandler()],\n",
    ")\n",
    "qa = RetrievalQA.from_chain_type(llm = openai,\n",
    "                                 chain_type = \"stuff\",\n",
    "                                 retriever = ensemble_retriever,\n",
    "                                 return_source_documents = True)\n",
    "\n",
    "query = \"7월 29일 카카오톡 내용\"\n",
    "result = qa(query)\n",
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LxPfaF0L0c2I"
   },
   "outputs": [],
   "source": [
    "for i in result['source_documents']:\n",
    "  print(i.metadata)\n",
    "  print(\"-\"*100)\n",
    "  print(i.page_content)\n",
    "  print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bxftciR52lYW"
   },
   "outputs": [],
   "source": [
    "faiss_vectorstore = FAISS.from_documents(docs, ko_embedding)\n",
    "faiss_retriever = faiss_vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(llm = openai,\n",
    "                                 chain_type = \"stuff\",\n",
    "                                 retriever = faiss_retriever,\n",
    "                                 return_source_documents = True)\n",
    "\n",
    "query = \"2024-07-28 에 [박상필]이 보낸 메세지\"\n",
    "result = qa(query)\n",
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kId5kM9H3LxY"
   },
   "outputs": [],
   "source": [
    "for i in result['source_documents']:\n",
    "  print(i.metadata)\n",
    "  print(\"-\"*100)\n",
    "  print(i.page_content)\n",
    "  print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MXZ7dGLm3e8A"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-YinIRx6_--y"
   },
   "source": [
    "## Long Context Reorder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "53bnfKrCAPxj"
   },
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain, StuffDocumentsChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.document_transformers import (\n",
    "    LongContextReorder,\n",
    ")\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "texts = [\n",
    "    \"바스켓볼은 훌륭한 스포츠입니다.\",\n",
    "    \"플라이 미 투 더 문은 제가 가장 좋아하는 노래 중 하나입니다.\",\n",
    "    \"셀틱스는 제가 가장 좋아하는 팀입니다.\",\n",
    "    \"보스턴 셀틱스에 관한 문서입니다.\", \"보스턴 셀틱스는 제가 가장 좋아하는 팀입니다.\",\n",
    "    \"저는 영화 보러 가는 것을 좋아해요\",\n",
    "    \"보스턴 셀틱스가 20점차로 이겼어요\",\n",
    "    \"이것은 그냥 임의의 텍스트입니다.\",\n",
    "    \"엘든 링은 지난 15 년 동안 최고의 게임 중 하나입니다.\",\n",
    "    \"L. 코넷은 최고의 셀틱스 선수 중 한 명입니다.\",\n",
    "    \"래리 버드는 상징적 인 NBA 선수였습니다.\",\n",
    "]\n",
    "\n",
    "# Create a retriever\n",
    "retriever = Chroma.from_texts(texts, embedding=ko_embedding).as_retriever(\n",
    "    search_kwargs={\"k\": 10}\n",
    ")\n",
    "query = \"셀틱스에 대해 어떤 이야기를 들려주시겠어요?\"\n",
    "\n",
    "# Get relevant documents ordered by relevance score\n",
    "docs = retriever.get_relevant_documents(query)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x_9sEUx0AX9h"
   },
   "outputs": [],
   "source": [
    "reordering = LongContextReorder()\n",
    "reordered_docs = reordering.transform_documents(docs)\n",
    "\n",
    "# Confirm that the 4 relevant documents are at beginning and end.\n",
    "reordered_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jxdYbGqgA4Sf"
   },
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain, StuffDocumentsChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = 'YOUR_API_KEY'\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "document_prompt = PromptTemplate(\n",
    "    input_variables=[\"page_content\"], template=\"{page_content}\"\n",
    ")\n",
    "\n",
    "template = \"\"\"Given this text extracts:\n",
    "-----\n",
    "{context}\n",
    "-----\n",
    "Please answer the following question:\n",
    "{query}\"\"\"\n",
    "prompt = PromptTemplate(\n",
    "    template=template, input_variables=[\"context\", \"query\"]\n",
    ")\n",
    "openai = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature = 0)\n",
    "\n",
    "llm_chain = LLMChain(llm=openai, prompt=prompt)\n",
    "chain = StuffDocumentsChain(\n",
    "    llm_chain=llm_chain,\n",
    "    document_prompt=document_prompt,\n",
    "    document_variable_name=\"context\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cn0oLiy3D_WR"
   },
   "outputs": [],
   "source": [
    "reordered_result = chain.run(input_documents=reordered_docs, query=query)\n",
    "result = chain.run(input_documents=docs, query=query)\n",
    "\n",
    "print(reordered_result)\n",
    "print(\"-\"*100)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JaIjA6LWEdP2"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "_HBhuL23S_YR",
    "digNJMyRtM3k",
    "AK752NU_tHjb",
    "FoTL9eBctWva",
    "zQP9iztA8ApB"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "rag"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
